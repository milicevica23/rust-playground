{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table_path = \"./tmp/delta-table\"\n",
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "# Update every even value by adding 100 to it\n",
    "deltaTable.update(\n",
    "  condition = expr(\"id % 2 == 0\"),\n",
    "  set = { \"id\": expr(\"id + 100\") })\n",
    "\n",
    "# Delete every even value\n",
    "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
    "\n",
    "# Upsert (merge) new data\n",
    "newData = spark.range(0, 20)\n",
    "\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\") \\\n",
    "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
    "  .execute()\n",
    "\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     14|2023-04-19 01:33:...|  null|    null|       MERGE|{predicate -> (ol...|null|    null|     null|         13|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     13|2023-04-19 01:33:...|  null|    null|      DELETE|{predicate -> [\"(...|null|    null|     null|         12|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     12|2023-04-19 01:33:...|  null|    null|      UPDATE|{predicate -> ((i...|null|    null|     null|         11|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     11|2023-04-19 01:29:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         10|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     10|2023-04-19 01:29:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          9|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|      9|2023-04-19 01:21:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          8|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      8|2023-04-19 01:21:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          7|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|      7|2023-04-19 01:15:...|  null|    null|    OPTIMIZE|{predicate -> [],...|null|    null|     null|          6|SnapshotIsolation|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      6|2023-04-19 01:13:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          5|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      5|2023-04-19 01:13:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          4|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      4|2023-04-19 01:12:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          3|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      3|2023-04-19 01:10:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          2|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-04-19 01:02:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          1|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-04-19 01:00:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          0|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-04-19 00:21:...|  null|    null|       WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|     Serializable|         true|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
    "fullHistoryDF.show()\n",
    "#lastOperationDF = deltaTable.history(1) # get the last operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "|format|                  id|name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|       tableFeatures|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "| delta|70c5aaf5-74e9-46e...|null|       null|file:/home/aleks/...|2023-04-19 01:45:...|2023-04-19 02:00:...|              []|       1|       4357|        {}|               1|               2|[appendOnly, inva...|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "detailDF = deltaTable.detail()\n",
    "detailDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, \"./tmp/delta-table\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 915 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)  # path-based tables, or\n",
    "#deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables\n",
    "\n",
    "deltaTable.vacuum(0)        # vacuum files not required by versions older than the default retention period\n",
    "\n",
    "#deltaTable.vacuum(100)     # vacuum files not required by versions more than 100 hours old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 01:45:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"./tmp/checkpoint\").start(\"./tmp/delta-table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 02:00:20 ERROR FileFormatWriter: Aborting job d0ba25d7-a813-4631-bbc7-bf941d25ca9a.\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:398)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.$anonfun$addBatch$1(DeltaSink.scala:95)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.$anonfun$addBatch$1$adapted(DeltaSink.scala:53)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.addBatch(DeltaSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:665)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:663)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:663)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 0.0 in stage 9061.0 (TID 137668) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 4.0 in stage 9061.0 (TID 137672) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 3.0 in stage 9061.0 (TID 137671) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 6.0 in stage 9061.0 (TID 137674) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 5.0 in stage 9061.0 (TID 137673) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 1.0 in stage 9061.0 (TID 137669) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 2.0 in stage 9061.0 (TID 137670) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 7.0 in stage 9061.0 (TID 137675) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 46|\n",
      "| 73|\n",
      "| 52|\n",
      "| 49|\n",
      "| 71|\n",
      "| 79|\n",
      "| 12|\n",
      "| 16|\n",
      "|  4|\n",
      "| 17|\n",
      "| 19|\n",
      "| 56|\n",
      "| 27|\n",
      "| 59|\n",
      "| 53|\n",
      "| 77|\n",
      "| 13|\n",
      "| 37|\n",
      "| 39|\n",
      "| 24|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
