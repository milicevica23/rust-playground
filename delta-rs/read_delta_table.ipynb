{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 23:26:34 WARN Utils: Your hostname, DESKTOP-1LBECNS resolves to a loopback address: 127.0.1.1; using 192.168.255.35 instead (on interface eth0)\n",
      "23/04/21 23:26:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/aleks/git/my-projects/rust-playground/delta-rs/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aleks/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aleks/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bd40fdac-efac-440b-abce-2591696138a4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 194ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bd40fdac-efac-440b-abce-2591696138a4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 23:26:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "        .config(\"spark.jars.packages\", \"\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 23:26:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|352|\n",
      "| 46|\n",
      "| 84|\n",
      "|220|\n",
      "|127|\n",
      "|191|\n",
      "| 73|\n",
      "|150|\n",
      "|292|\n",
      "|401|\n",
      "| 52|\n",
      "|390|\n",
      "|253|\n",
      "|229|\n",
      "|208|\n",
      "| 49|\n",
      "|216|\n",
      "| 89|\n",
      "| 71|\n",
      "| 79|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table_path = \"./tmp/delta-table\"\n",
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/19 01:33:08 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "# Update every even value by adding 100 to it\n",
    "deltaTable.update(\n",
    "  condition = expr(\"id % 2 == 0\"),\n",
    "  set = { \"id\": expr(\"id + 100\") })\n",
    "\n",
    "# Delete every even value\n",
    "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
    "\n",
    "# Upsert (merge) new data\n",
    "newData = spark.range(0, 20)\n",
    "\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\") \\\n",
    "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
    "  .execute()\n",
    "\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     14|2023-04-19 01:33:...|  null|    null|       MERGE|{predicate -> (ol...|null|    null|     null|         13|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     13|2023-04-19 01:33:...|  null|    null|      DELETE|{predicate -> [\"(...|null|    null|     null|         12|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     12|2023-04-19 01:33:...|  null|    null|      UPDATE|{predicate -> ((i...|null|    null|     null|         11|     Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     11|2023-04-19 01:29:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         10|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|     10|2023-04-19 01:29:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          9|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|      9|2023-04-19 01:21:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          8|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      8|2023-04-19 01:21:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          7|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.3....|\n",
      "|      7|2023-04-19 01:15:...|  null|    null|    OPTIMIZE|{predicate -> [],...|null|    null|     null|          6|SnapshotIsolation|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      6|2023-04-19 01:13:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          5|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      5|2023-04-19 01:13:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          4|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      4|2023-04-19 01:12:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          3|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      3|2023-04-19 01:10:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          2|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-04-19 01:02:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          1|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-04-19 01:00:...|  null|    null|       WRITE|{mode -> Overwrit...|null|    null|     null|          0|     Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-04-19 00:21:...|  null|    null|       WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|     Serializable|         true|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
    "fullHistoryDF.show()\n",
    "#lastOperationDF = deltaTable.history(1) # get the last operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "|format|                  id|name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|       tableFeatures|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "| delta|70c5aaf5-74e9-46e...|null|       null|file:/home/aleks/...|2023-04-19 01:45:...|2023-04-19 02:00:...|              []|       1|       4357|        {}|               1|               2|[appendOnly, inva...|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)\n",
    "\n",
    "detailDF = deltaTable.detail()\n",
    "detailDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, \"./tmp/delta-table\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 915 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, table_path)  # path-based tables, or\n",
    "#deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables\n",
    "\n",
    "deltaTable.vacuum(0)        # vacuum files not required by versions older than the default retention period\n",
    "\n",
    "#deltaTable.vacuum(100)     # vacuum files not required by versions more than 100 hours old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 01:45:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"./tmp/checkpoint\").start(\"./tmp/delta-table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 02:00:20 ERROR FileFormatWriter: Aborting job d0ba25d7-a813-4631-bbc7-bf941d25ca9a.\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:255)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:398)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.$anonfun$addBatch$1(DeltaSink.scala:95)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.$anonfun$addBatch$1$adapted(DeltaSink.scala:53)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSink.addBatch(DeltaSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:665)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:663)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:663)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 0.0 in stage 9061.0 (TID 137668) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 4.0 in stage 9061.0 (TID 137672) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 3.0 in stage 9061.0 (TID 137671) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 6.0 in stage 9061.0 (TID 137674) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 5.0 in stage 9061.0 (TID 137673) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 1.0 in stage 9061.0 (TID 137669) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 2.0 in stage 9061.0 (TID 137670) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/19 02:00:20 WARN TaskSetManager: Lost task 7.0 in stage 9061.0 (TID 137675) (192.168.250.148 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 46|\n",
      "| 73|\n",
      "| 52|\n",
      "| 49|\n",
      "| 71|\n",
      "| 79|\n",
      "| 12|\n",
      "| 16|\n",
      "|  4|\n",
      "| 17|\n",
      "| 19|\n",
      "| 56|\n",
      "| 27|\n",
      "| 59|\n",
      "| 53|\n",
      "| 77|\n",
      "| 13|\n",
      "| 37|\n",
      "| 39|\n",
      "| 24|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(table_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|city_id|\n",
      "+-----+---+-------+\n",
      "|aleks| 10|      1|\n",
      "|burcu| 20|      2|\n",
      "| maja| 20|      1|\n",
      "+-----+---+-------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- city_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_path = \"./tmp/csv_data/person.csv\"\n",
    "personsDf = spark.read.csv(path=p_path,header=True,)\n",
    "personsDf.show()\n",
    "personsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|city_id|city_name|\n",
      "+-------+---------+\n",
      "|      1| Belgrade|\n",
      "|      2| Istanbul|\n",
      "+-------+---------+\n",
      "\n",
      "root\n",
      " |-- city_id: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_path = \"./tmp/csv_data/city.csv\"\n",
    "cytyDf = spark.read.csv(path=c_path,header=True)\n",
    "cytyDf.show()\n",
    "cytyDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|burcu|\n",
      "| maja|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDf.filter(personsDf[\"age\"]> 15).select(personsDf[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|city_id|count|\n",
      "+-------+-----+\n",
      "|      1|    2|\n",
      "|      2|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDf.groupBy(personsDf[\"city_id\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+---------+\n",
      "| name|age|city_id|city_id|city_name|\n",
      "+-----+---+-------+-------+---------+\n",
      "|aleks| 10|      1|      1| Belgrade|\n",
      "|burcu| 20|      2|      2| Istanbul|\n",
      "| maja| 20|      1|      1| Belgrade|\n",
      "+-----+---+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDf.join(cytyDf, personsDf[\"city_id\"] == cytyDf[\"city_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "personsDf.write.format(\"delta\").save(\"./tmp/persons-delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- city_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_df = spark.read.format(\"delta\").load(\"./tmp/persons-delta/\")\n",
    "#p_df.show()\n",
    "p_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|  name|age|city_id|\n",
      "+------+---+-------+\n",
      "|zeljko| 25|      3|\n",
      "| burcu| 18|      2|\n",
      "|  nata| 13|      1|\n",
      "+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_df2 = spark.read.csv(\"./tmp/csv_data/person_new.csv\",header=True)\n",
    "p_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|city_id|\n",
      "+-----+---+-------+\n",
      "|aleks| 10|      1|\n",
      "|burcu| 20|      2|\n",
      "| maja| 20|      1|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "personsDf.write.saveAsTable(\"persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+\n",
      "|location                                                                         |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|file:/home/aleks/git/my-projects/rust-playground/delta-rs/spark-warehouse/persons|\n",
      "+---------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE DETAIL persons').select(\"location\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_T = DeltaTable.forPath(spark, \"./tmp/persons-delta/\") \n",
    "delta_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|  name|age|city_id|\n",
      "+------+---+-------+\n",
      "| burcu| 18|      2|\n",
      "|  maja| 25|      1|\n",
      "|  nata| 13|      1|\n",
      "|zeljko| 25|      3|\n",
      "+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_T.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                        |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|3      |2023-04-22 10:06:46.738|null  |null    |MERGE    |{predicate -> (new.name = old.name), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|null    |null     |2          |Serializable  |false        |{numTargetRowsCopied -> 1, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1014, numTargetBytesRemoved -> 902, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 924, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 551, numTargetRowsUpdated -> 1, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 336}|null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|2      |2023-04-22 09:48:38.528|null  |null    |UPDATE   |{predicate -> (name#2423 = maja)}                                                                                                                                          |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 939, numCopiedRows -> 1, numAddedChangeFiles -> 0, executionTimeMs -> 528, scanTimeMs -> 363, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 902, rewriteTimeMs -> 164}                                                                                                                                                                                                                                                                                                        |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|1      |2023-04-22 09:43:26.678|null  |null    |DELETE   |{predicate -> [\"(CAST(age AS INT) < 13)\"]}                                                                                                                                 |null|null    |null     |0          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 982, numCopiedRows -> 2, numAddedChangeFiles -> 0, executionTimeMs -> 749, numDeletedRows -> 1, scanTimeMs -> 562, numAddedFiles -> 1, numAddedBytes -> 939, rewriteTimeMs -> 187}                                                                                                                                                                                                                                                                                                        |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "|0      |2023-04-22 00:15:27.478|null  |null    |WRITE    |{mode -> ErrorIfExists, partitionBy -> []}                                                                                                                                 |null|null    |null     |null       |Serializable  |true         |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 982}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.3.2 Delta-Lake/2.3.0|\n",
      "+-------+-----------------------+------+--------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_T.history().show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|  name|age|city_id|\n",
      "+------+---+-------+\n",
      "|zeljko| 25|      3|\n",
      "| burcu| 18|      2|\n",
      "|  nata| 13|      1|\n",
      "+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|city_id|\n",
      "+-----+---+-------+\n",
      "|burcu| 20|      2|\n",
      "| maja| 25|      1|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 10:06:46 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/22 10:06:46 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/22 10:06:46 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n"
     ]
    }
   ],
   "source": [
    "p_dt = DeltaTable.forPath(spark,\"./tmp/persons-delta/\")\n",
    "p_dt.alias(\"old\").merge(\n",
    "    source= p_df2.alias(\"new\"),\n",
    "    condition= \"new.name = old.name\"\n",
    ")\\\n",
    ".whenMatchedUpdateAll()\\\n",
    ".whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m wiki_stream \u001b[39m=\u001b[39m spark\\\n\u001b[1;32m      2\u001b[0m     \u001b[39m.\u001b[39;49mreadStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mkafka\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mkafka.bootstrap.servers\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhttps://stream.wikimedia.org/v2/stream/mediawiki.recentchange\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39msubscribe\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39meqiad.mediawiki.recentchange\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m----> 6\u001b[0m     \u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/git/my-projects/rust-playground/delta-rs/.venv/lib/python3.11/site-packages/pyspark/sql/streaming.py:469\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(path))\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m~/git/my-projects/rust-playground/delta-rs/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/git/my-projects/rust-playground/delta-rs/.venv/lib/python3.11/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        "
     ]
    }
   ],
   "source": [
    "wiki_stream = spark\\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"https://stream.wikimedia.org/v2/stream/mediawiki.recentchange\") \\\n",
    "    .option(\"subscribe\",\"eqiad.mediawiki.recentchange\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
